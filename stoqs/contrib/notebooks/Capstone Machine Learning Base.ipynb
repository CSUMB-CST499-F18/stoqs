{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Capstone Machine Learning Base"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created by Bret  Stine, Mark Mocek, and Miranda Saari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilize basic data exploration and machine learning techniques to classify plankton."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do we include this part? from classify_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing this notebook requires a personal STOQS database. Follow the steps to [build your own development system](https://github.com/stoqs/stoqs/blob/master/README.md), this will take about an hour or so depending on the quality of your internet connection. Once your server is follow the proceeding step to get your virtual environment up and running:\n",
    "    \n",
    "    cd ~/Vagrants/stoqsvm\n",
    "    vagrant ssh -- -X\n",
    "    cd /vagrant/dev/stoqsgit\n",
    "    source venv-stoqs/bin/activate\n",
    "    \n",
    "Then load the chosen database (ex:`stoqs_september2013`) database with the commands:\n",
    "\n",
    "    cd stoqs\n",
    "    ln -s mbari_campaigns.py campaigns.py\n",
    "    export DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs\n",
    "    loaders/load.py --db stoqs_september2013\n",
    "    loaders/load.py --db stoqs_september2013 --updateprovenance\n",
    "   \n",
    "Loading this database can take over a day as there are over 40 million measurements from 22 different platforms. You may want to edit the `stoqs/loaders/CANON/loadCANON_september2013.py` file and comment all but the `loadDorado()` method calls at the end of the file. You can also set a stride value or use the `--test` option to create a `stoqs_september2013_t` database, in which case you'll need to set the STOQS_CAMPAIGNS envrironment variable: \n",
    "\n",
    "    export STOQS_CAMPAIGNS=stoqs_september2013_t\n",
    "\n",
    "Use the `stoqs/contrib/analysis/classify.py` script to create some labeled data that we will learn from:\n",
    "\n",
    "    contrib/analysis/classify.py --createLabels --groupName Plankton \\\n",
    "        --database stoqs_september2013 --platform dorado \\\n",
    "        --start 20130916T124035 --end 20130919T233905 \\\n",
    "        --inputs bbp700 fl700_uncorr --discriminator salinity \\\n",
    "        --labels diatom dino1 dino2 sediment \\\n",
    "        --mins 33.33 33.65 33.70 33.75 --maxes 33.65 \n",
    "        33.70 33.75 33.93 --clobber -v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Executing notebooks after installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start Xming\n",
    "Open a putty window\n",
    "        `cd dev/stoqsgit && source venv-stoqs/bi/activate`\n",
    "        `export DATABASE_URL=postgis://stoqsadm:CHANGEME@127.0.0.1:5438/stoqs`\n",
    "        `export STOQS_CAMPAIGNS=stoqs_september2013_t`\n",
    "        `cd stoqs/contrib/notebooks`\n",
    "        `../../manage.py shell_plus --notebook`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Libraries\n",
    "to use seaborn, must run `pip install seaborn` on the instance which is running the notebooks then rerun the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd \n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import time, datetime\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the stoqs data into a pandas data frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To find other parameters to put into your data frame, look at other paramaters by going to http://localhost:8008/stoqs_september2013_o/api/[table_name_here] where your STOQS server is running. Note: if the parameters are changed, the findings of this notebook may no longer correlate. We suggest only doing so for the use of your own notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mps = MeasuredParameter.objects.using('stoqs_september2013_o').filter(measurement__instantpoint__activity__platform__name='dorado')\n",
    "# df = pd.DataFrame.from_records(mps.values('measurement__instantpoint__timevalue', 'measurement__depth',\n",
    "#                                           'measurement__geom', 'parameter__name', 'datavalue', 'id'))\n",
    "df = pd.DataFrame.from_records(mps.values('measurement__instantpoint__timevalue', 'measurement__depth', \n",
    "                                          'measurement__geom', 'parameter__name', 'datavalue', 'id', \n",
    "                                          'measuredparameterresource__resource__value'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Bret and McCann's way"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# mps = MeasuredParameter.objects.using('stoqs_september2013_o').filter(\n",
    "#   measurement__instantpoint__activity__platform__name='dorado')\n",
    "# #mps = mps.filter(measuredparameterresource__resource__name='diatom')\n",
    "# df = pd.DataFrame.from_records(mps.values('measurement__instantpoint__timevalue', 'measurement__depth', \n",
    "#                      'measurement__geom', 'parameter__name', 'datavalue', 'id', 'measuredparameterresource__resource__value'))\n",
    "\n",
    "# df[0:100]\n",
    "\n",
    "# print(df['parameter__name'].unique())\n",
    "# print(df['measuredparameterresource__resource__value'].unique())\n",
    "\n",
    "# rs = Resource.objects.using('stoqs_september2013_o').filter(value='diatom')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploring Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "stoqs_september2013_o dataset contains 849,935 rows of data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Original Column Names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the original column names were quite lengthy, I renamed them to a simpler name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.columns=['value', 'id', 'resourceValue', 'depth', 'geom', 'time', 'name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking into the number of null values in the data, we found out of the used paramaters, value and resourceValue are the only columns to contatin null, but resourceValue is all null. After further examination, I found this is only from my machine, from Bret's the labels are properly applied."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.isnull().sum())\n",
    "print(df['resourceValue'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By looking at the first and last row of data we see the collection of data started at 3:50:48pm on September 9,2013 and ended on 8:07:44 PM on October 3, 2013. We may consider looking at the data in chunks of time since the AUVs move through the water and the data collected in one part of the water may not correlate to the data in another part. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.time.loc[[0]])\n",
    "print (df.time.loc[[849934]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the almost equal dispersement of each data sample, we assume every sample contains each of these 14 values. Thus with the the 849,935 rows of data really 60,254 for each parameter was used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.name.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.resourceValue.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I then thought it would be interesting to see how many measurements were taking at various depths. This shows minimum and maximum depth along with counts at the various depths. Since the measurements at different depths can bring different results I think it would be helpful to examine outcomes among similar depths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(min(df.depth))\n",
    "print(max(df.depth))\n",
    "print(df.depth.value_counts().head())\n",
    "df.depth.value_counts().tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another parameter to sort between would be latitude and longitude to see how the patterns change as the AUV moves. The following extracts the latitude and longitude into two columns and then renames them properly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = df.join(pd.DataFrame(df['geom'].values.tolist(), index=df.index))\n",
    "df.rename(columns={0:'latitude', 1:'longitude'}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Plotting parameters over time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think it would be good to find a way to graph the patterns over time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting to looking into parameters over time, here is altitude over time. By getting better at working with time data I think we should 'zoom in' on the amount of time we are looking at."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot('time', 'value', data=df[df.name==\"altitude\"][0:1000])\n",
    "plt.title(\"Altitude over time\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Looking into outcomes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: this does not mean much for me as I only see 'none' as an outcome, temporarily will assign outcomes. Do not run the following code if your data is already labeled. Also recieved warnings for the assignments so temporarily disabled them. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df.shape\n",
    "#possible outcomes(counts according to Bret): \n",
    "#'diatom'(9182) 'dino1'(20474) 'dino2'(13176) 'sediment'(6760)\n",
    "pd.options.mode.chained_assignment = None  # default='warn'\n",
    "df.resourceValue[0:9182]='diatom'\n",
    "df.resourceValue[9182:29656]='dino1'\n",
    "df.resourceValue[29656:42832]='dino2'\n",
    "df.resourceValue[42832:849935]='sediment'\n",
    "df.resourceValue.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, I thought we should see the dispertion of paramaters labeled as each resource value. From here we could see the trend in values. Since  these values were classified by given parameters such as salinity and ______ (I forget the other they are using), I would like to see if any other values align with these to predict these outcomes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Quick example of barplot of values in a column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.value_counts(df['resourceValue']).plot.bar()\n",
    "plt.title(\"Counts of all outcomes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thoughts on possible problems\n",
    "* each label is associated with a group of value names\n",
    "* labels only exist for particular value names and in that case some value names only have 'None' as a label\n",
    "with that being said, not going to graph more value names until I know more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "pd.value_counts(df[df.name=='altitude'].resourceValue).plot.bar()\n",
    "plt.title(\"Outcomes by altitude\")\n",
    "print(df[df.name=='altitude'].resourceValue.value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Thoughts on things to explore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* collection of variables at particular depth (ex: value of altitude at -0.030662) only issue, when I do df[df.depth=='0.030662] I get nothing, so would need to fix this. I chose this depth bc it has the most rows of data\n",
    "* graph the geom x time (was trying to split the latitude and longitude up to do this, thought it would be cool to see the path the auv takes through the water)\n",
    "* incorporate value/depth/time into a graph. \n",
    "\n",
    "Thoughts: since values are only relative to certain variables I think it is best to go by the variable names (i.e. altitude, sigmant, spice, etc.) instead of graphing them as one entity. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Django Shell-Plus",
   "language": "python",
   "name": "django_extensions"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
